{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-05T17:31:10.373606Z","iopub.status.busy":"2023-11-05T17:31:10.373286Z","iopub.status.idle":"2023-11-05T17:31:13.521662Z","shell.execute_reply":"2023-11-05T17:31:13.520373Z","shell.execute_reply.started":"2023-11-05T17:31:10.373577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'detox'...\n","remote: Enumerating objects: 186, done.\u001b[K\n","remote: Counting objects: 100% (186/186), done.\u001b[K\n","remote: Compressing objects: 100% (164/164), done.\u001b[K\n","remote: Total 186 (delta 54), reused 118 (delta 14), pack-reused 0\u001b[K\n","Receiving objects: 100% (186/186), 17.92 MiB | 18.55 MiB/s, done.\n","Resolving deltas: 100% (54/54), done.\n"]}],"source":["!git clone https://github.com/s-nlp/detox"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:31:22.557811Z","iopub.status.busy":"2023-11-05T17:31:22.557446Z","iopub.status.idle":"2023-11-05T17:33:42.457473Z","shell.execute_reply":"2023-11-05T17:33:42.456418Z","shell.execute_reply.started":"2023-11-05T17:31:22.557779Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 3.10.1 which is incompatible.\n","dask 2023.7.0 requires importlib-metadata>=4.13.0, but you have importlib-metadata 3.10.1 which is incompatible.\n","dask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\n","dask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\n","distributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\n","google-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 3.1.0 which is incompatible.\n","jupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","keyring 24.2.0 requires importlib-metadata>=4.11.4; python_version < \"3.12\", but you have importlib-metadata 3.10.1 which is incompatible.\n","opentelemetry-api 1.18.0 requires importlib-metadata~=6.0.0, but you have importlib-metadata 3.10.1 which is incompatible.\n","raft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\n","yapf 0.40.1 requires importlib-metadata>=6.6.0, but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["! pip install -r detox/requirements.txt -q"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:33:52.527465Z","iopub.status.busy":"2023-11-05T17:33:52.527075Z","iopub.status.idle":"2023-11-05T17:33:52.534368Z","shell.execute_reply":"2023-11-05T17:33:52.533476Z","shell.execute_reply.started":"2023-11-05T17:33:52.527428Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","\n","def add_sys_path(p):\n","    p = os.path.abspath(p)\n","    print(p)\n","    if p not in sys.path:\n","        sys.path.append(p)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:33:55.654642Z","iopub.status.busy":"2023-11-05T17:33:55.654018Z","iopub.status.idle":"2023-11-05T17:33:55.659437Z","shell.execute_reply":"2023-11-05T17:33:55.658456Z","shell.execute_reply.started":"2023-11-05T17:33:55.654609Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/detox/emnlp2021/style_transfer/condBERT\n"]}],"source":["# adding the path to the condebert code root\n","add_sys_path('detox/emnlp2021/style_transfer/condBERT')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:34:06.510130Z","iopub.status.busy":"2023-11-05T17:34:06.509746Z","iopub.status.idle":"2023-11-05T17:34:20.627573Z","shell.execute_reply":"2023-11-05T17:34:20.626804Z","shell.execute_reply.started":"2023-11-05T17:34:06.510098Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["from importlib import reload\n","import condbert\n","reload(condbert)\n","from condbert import CondBertRewriter\n","import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","import numpy as np\n","import pickle\n","from tqdm.auto import tqdm, trange"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:34:26.247650Z","iopub.status.busy":"2023-11-05T17:34:26.247293Z","iopub.status.idle":"2023-11-05T17:34:26.253874Z","shell.execute_reply":"2023-11-05T17:34:26.252934Z","shell.execute_reply.started":"2023-11-05T17:34:26.247617Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cpu')"]},{"cell_type":"markdown","metadata":{},"source":["### Condbert Compile Vocab"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:34:59.768066Z","iopub.status.busy":"2023-11-05T17:34:59.767318Z","iopub.status.idle":"2023-11-05T17:34:59.772208Z","shell.execute_reply":"2023-11-05T17:34:59.771174Z","shell.execute_reply.started":"2023-11-05T17:34:59.768028Z"},"trusted":true},"outputs":[],"source":["VOCAB_DIRNAME = 'vocabularies' "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:35:09.309720Z","iopub.status.busy":"2023-11-05T17:35:09.309009Z","iopub.status.idle":"2023-11-05T17:35:10.676950Z","shell.execute_reply":"2023-11-05T17:35:10.676090Z","shell.execute_reply.started":"2023-11-05T17:35:09.309682Z"},"trusted":true},"outputs":[],"source":["from condbert import CondBertRewriter\n","from choosers import EmbeddingSimilarityChooser\n","from multiword.masked_token_predictor_bert import MaskedTokenPredictorBert"]},{"cell_type":"markdown","metadata":{},"source":["#### Load Bert"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:35:35.076815Z","iopub.status.busy":"2023-11-05T17:35:35.076436Z","iopub.status.idle":"2023-11-05T17:35:35.081569Z","shell.execute_reply":"2023-11-05T17:35:35.080569Z","shell.execute_reply.started":"2023-11-05T17:35:35.076781Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForMaskedLM\n","import numpy as np\n","import pickle\n","import os\n","from tqdm.auto import tqdm, trange"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:36:13.760938Z","iopub.status.busy":"2023-11-05T17:36:13.759906Z","iopub.status.idle":"2023-11-05T17:36:13.766359Z","shell.execute_reply":"2023-11-05T17:36:13.765120Z","shell.execute_reply.started":"2023-11-05T17:36:13.760873Z"},"trusted":true},"outputs":[],"source":["os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","device = torch.device('cuda:0')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:36:25.638214Z","iopub.status.busy":"2023-11-05T17:36:25.637880Z","iopub.status.idle":"2023-11-05T17:36:26.422709Z","shell.execute_reply":"2023-11-05T17:36:26.421791Z","shell.execute_reply.started":"2023-11-05T17:36:25.638178Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f5891d5ac8e482592adbbda30e204f9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75d5e5107f7841698a52d6b0d0e219c2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b041a8d1d10f4d8d824ccf2c8b3ae1a7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:36:38.902447Z","iopub.status.busy":"2023-11-05T17:36:38.902068Z","iopub.status.idle":"2023-11-05T17:36:42.859611Z","shell.execute_reply":"2023-11-05T17:36:42.858798Z","shell.execute_reply.started":"2023-11-05T17:36:38.902416Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4c5fa14f7264ead8ab7d2473ad150c9","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = BertForMaskedLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:36:50.814352Z","iopub.status.busy":"2023-11-05T17:36:50.813988Z","iopub.status.idle":"2023-11-05T17:36:56.521034Z","shell.execute_reply":"2023-11-05T17:36:56.520220Z","shell.execute_reply.started":"2023-11-05T17:36:50.814322Z"},"trusted":true},"outputs":[],"source":["model.to(device);"]},{"cell_type":"markdown","metadata":{},"source":["#### Preparing the vocabularires"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:44:45.415156Z","iopub.status.busy":"2023-11-05T17:44:45.414705Z","iopub.status.idle":"2023-11-05T17:44:45.420501Z","shell.execute_reply":"2023-11-05T17:44:45.419411Z","shell.execute_reply.started":"2023-11-05T17:44:45.415120Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists(VOCAB_DIRNAME):\n","    os.makedirs(VOCAB_DIRNAME)"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.1 Preparing the DRG-like vocabularies"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:44:49.800829Z","iopub.status.busy":"2023-11-05T17:44:49.799946Z","iopub.status.idle":"2023-11-05T17:44:50.219913Z","shell.execute_reply":"2023-11-05T17:44:50.219072Z","shell.execute_reply.started":"2023-11-05T17:44:49.800789Z"},"trusted":true},"outputs":[],"source":["import os\n","import argparse\n","import numpy as np\n","from tqdm import tqdm\n","from nltk import ngrams\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","\n","\n","\n","class NgramSalienceCalculator():\n","    def __init__(self, tox_corpus, norm_corpus, use_ngrams=False):\n","        ngrams = (1, 3) if use_ngrams else (1, 1)\n","        self.vectorizer = CountVectorizer(ngram_range=ngrams)\n","\n","        tox_count_matrix = self.vectorizer.fit_transform(tox_corpus)\n","        self.tox_vocab = self.vectorizer.vocabulary_\n","        self.tox_counts = np.sum(tox_count_matrix, axis=0)\n","\n","        norm_count_matrix = self.vectorizer.fit_transform(norm_corpus)\n","        self.norm_vocab = self.vectorizer.vocabulary_\n","        self.norm_counts = np.sum(norm_count_matrix, axis=0)\n","\n","    def salience(self, feature, attribute='tox', lmbda=0.5):\n","        assert attribute in ['tox', 'norm']\n","        if feature not in self.tox_vocab:\n","            tox_count = 0.0\n","        else:\n","            tox_count = self.tox_counts[0, self.tox_vocab[feature]]\n","\n","        if feature not in self.norm_vocab:\n","            norm_count = 0.0\n","        else:\n","            norm_count = self.norm_counts[0, self.norm_vocab[feature]]\n","\n","        if attribute == 'tox':\n","            return (tox_count + lmbda) / (norm_count + lmbda)\n","        else:\n","            return (norm_count + lmbda) / (tox_count + lmbda)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:46:13.759294Z","iopub.status.busy":"2023-11-05T17:46:13.758375Z","iopub.status.idle":"2023-11-05T17:46:16.187826Z","shell.execute_reply":"2023-11-05T17:46:16.186878Z","shell.execute_reply.started":"2023-11-05T17:46:13.759255Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv('../data/interim/data2.csv')"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:47:28.264496Z","iopub.status.busy":"2023-11-05T17:47:28.263628Z","iopub.status.idle":"2023-11-05T17:47:28.269013Z","shell.execute_reply":"2023-11-05T17:47:28.267929Z","shell.execute_reply.started":"2023-11-05T17:47:28.264460Z"},"trusted":true},"outputs":[],"source":["tox = df['toxic']\n","neu = df['neutral']"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:58:07.534097Z","iopub.status.busy":"2023-11-05T17:58:07.533712Z","iopub.status.idle":"2023-11-05T17:58:07.538695Z","shell.execute_reply":"2023-11-05T17:58:07.537697Z","shell.execute_reply.started":"2023-11-05T17:58:07.534064Z"},"trusted":true},"outputs":[],"source":["neg_out_name = \"../detoxs_rus/emnlp2021/style_transfer/condBERT/vocab/negative-words.txt\"\n","pos_out_name = \"../detoxs_rus/emnlp2021/style_transfer/condBERT/vocab/positive-words.txt\""]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:58:09.814231Z","iopub.status.busy":"2023-11-05T17:58:09.813860Z","iopub.status.idle":"2023-11-05T17:58:15.588047Z","shell.execute_reply":"2023-11-05T17:58:15.587058Z","shell.execute_reply.started":"2023-11-05T17:58:09.814194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["281442\n"]}],"source":["from collections import Counter\n","c = Counter()\n","\n","with open(neg_out_name, 'r') as neg_out, open(pos_out_name, 'r') as pos_out:\n","    existant_pos_words = pos_out.readlines()\n","    for line in existant_pos_words:\n","        for tok in line.strip().split():\n","            c[tok] += 1\n","    existant_neg_words = neg_out.readlines()\n","    for line in existant_neg_words:\n","        for tok in line.strip().split():\n","            c[tok] += 1\n","\n","for fn in [tox, neu]:\n","        for line in fn:\n","            for tok in line.strip().split():\n","                c[tok] += 1\n","print(len(c))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T17:58:20.438467Z","iopub.status.busy":"2023-11-05T17:58:20.438086Z","iopub.status.idle":"2023-11-05T17:58:20.600578Z","shell.execute_reply":"2023-11-05T17:58:20.599511Z","shell.execute_reply.started":"2023-11-05T17:58:20.438435Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["281442\n"]}],"source":["vocab = {w for w, _ in c.most_common() if _ > 0}  # if we took words with > 1 occurences, vocabulary would be x2 smaller, but we'll survive this size\n","print(len(vocab))"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:00:55.566583Z","iopub.status.busy":"2023-11-05T18:00:55.565544Z","iopub.status.idle":"2023-11-05T18:00:59.550856Z","shell.execute_reply":"2023-11-05T18:00:59.549807Z","shell.execute_reply.started":"2023-11-05T18:00:55.566534Z"},"trusted":true},"outputs":[],"source":["# with open(tox_corpus_path, 'r') as tox_corpus, open(norm_corpus_path, 'r') as norm_corpus:\n","corpus_tox = [' '.join([w if w in vocab else '<unk>' for w in line.strip().split()]) for line in tox]\n","corpus_norm = [' '.join([w if w in vocab else '<unk>' for w in line.strip().split()]) for line in neu]"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:01:05.917973Z","iopub.status.busy":"2023-11-05T18:01:05.917488Z","iopub.status.idle":"2023-11-05T18:01:05.922583Z","shell.execute_reply":"2023-11-05T18:01:05.921550Z","shell.execute_reply.started":"2023-11-05T18:01:05.917939Z"},"trusted":true},"outputs":[],"source":["neg_out_name = VOCAB_DIRNAME + '/negative-words.txt'\n","pos_out_name = VOCAB_DIRNAME + '/positive-words.txt'"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:01:09.021374Z","iopub.status.busy":"2023-11-05T18:01:09.020502Z","iopub.status.idle":"2023-11-05T18:01:09.025208Z","shell.execute_reply":"2023-11-05T18:01:09.024284Z","shell.execute_reply.started":"2023-11-05T18:01:09.021325Z"},"trusted":true},"outputs":[],"source":["threshold = 4"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:01:15.464457Z","iopub.status.busy":"2023-11-05T18:01:15.464054Z","iopub.status.idle":"2023-11-05T18:01:34.329242Z","shell.execute_reply":"2023-11-05T18:01:34.328390Z","shell.execute_reply.started":"2023-11-05T18:01:15.464423Z"},"trusted":true},"outputs":[],"source":["sc = NgramSalienceCalculator(corpus_tox, corpus_norm, False)\n","seen_grams = set()\n","\n","with open(neg_out_name, 'w') as neg_out, open(pos_out_name, 'w') as pos_out:\n","    for gram in set(sc.tox_vocab.keys()).union(set(sc.norm_vocab.keys())):\n","        if gram not in seen_grams:\n","            seen_grams.add(gram)\n","            toxic_salience = sc.salience(gram, attribute='tox')\n","            polite_salience = sc.salience(gram, attribute='norm')\n","            if toxic_salience > threshold:\n","                neg_out.writelines(f'{gram}\\n')\n","            elif polite_salience > threshold:\n","                pos_out.writelines(f'{gram}\\n')"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2 Evaluating word toxicities with a logistic regression"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:07:33.237644Z","iopub.status.busy":"2023-11-05T18:07:33.237248Z","iopub.status.idle":"2023-11-05T18:07:33.251767Z","shell.execute_reply":"2023-11-05T18:07:33.250650Z","shell.execute_reply.started":"2023-11-05T18:07:33.237614Z"},"trusted":true},"outputs":[],"source":["from sklearn.pipeline import make_pipeline\n","pipe = make_pipeline(CountVectorizer(), LogisticRegression(max_iter=10000))"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:07:35.461764Z","iopub.status.busy":"2023-11-05T18:07:35.460986Z","iopub.status.idle":"2023-11-05T18:11:40.475427Z","shell.execute_reply":"2023-11-05T18:11:40.473739Z","shell.execute_reply.started":"2023-11-05T18:07:35.461719Z"},"trusted":true},"outputs":[],"source":["X_train = corpus_tox + corpus_norm\n","y_train = [1] * len(corpus_tox) + [0] * len(corpus_norm)\n","pipe.fit(X_train, y_train);"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:12:03.436074Z","iopub.status.busy":"2023-11-05T18:12:03.435457Z","iopub.status.idle":"2023-11-05T18:12:03.444671Z","shell.execute_reply":"2023-11-05T18:12:03.443638Z","shell.execute_reply.started":"2023-11-05T18:12:03.436010Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(101070,)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["coefs = pipe[1].coef_[0]\n","coefs.shape"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:12:07.398340Z","iopub.status.busy":"2023-11-05T18:12:07.397914Z","iopub.status.idle":"2023-11-05T18:12:07.458740Z","shell.execute_reply":"2023-11-05T18:12:07.457513Z","shell.execute_reply.started":"2023-11-05T18:12:07.398290Z"},"trusted":true},"outputs":[],"source":["word2coef = {w: coefs[idx] for w, idx in pipe[0].vocabulary_.items()}"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:12:11.543541Z","iopub.status.busy":"2023-11-05T18:12:11.542637Z","iopub.status.idle":"2023-11-05T18:12:11.882290Z","shell.execute_reply":"2023-11-05T18:12:11.881412Z","shell.execute_reply.started":"2023-11-05T18:12:11.543503Z"},"trusted":true},"outputs":[],"source":["import pickle\n","with open(VOCAB_DIRNAME + '/word2coef.pkl', 'wb') as f:\n","    pickle.dump(word2coef, f)"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.3 Labelling BERT tokens by toxicity"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:12:15.110299Z","iopub.status.busy":"2023-11-05T18:12:15.109231Z","iopub.status.idle":"2023-11-05T18:21:26.765543Z","shell.execute_reply":"2023-11-05T18:21:26.764527Z","shell.execute_reply.started":"2023-11-05T18:12:15.110258Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 577777/577777 [04:37<00:00, 2080.97it/s]\n","100%|██████████| 577777/577777 [04:33<00:00, 2108.71it/s]\n"]}],"source":["from collections import defaultdict\n","toxic_counter = defaultdict(lambda: 1)\n","nontoxic_counter = defaultdict(lambda: 1)\n","\n","for text in tqdm(corpus_tox):\n","    for token in tokenizer.encode(text):\n","        toxic_counter[token] += 1\n","for text in tqdm(corpus_norm):\n","    for token in tokenizer.encode(text):\n","        nontoxic_counter[token] += 1"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:21:32.112532Z","iopub.status.busy":"2023-11-05T18:21:32.111737Z","iopub.status.idle":"2023-11-05T18:21:32.130110Z","shell.execute_reply":"2023-11-05T18:21:32.129129Z","shell.execute_reply.started":"2023-11-05T18:21:32.112496Z"},"trusted":true},"outputs":[],"source":["token_toxicities = [toxic_counter[i] / (nontoxic_counter[i] + toxic_counter[i]) for i in range(len(tokenizer.vocab))]"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:21:38.260036Z","iopub.status.busy":"2023-11-05T18:21:38.259312Z","iopub.status.idle":"2023-11-05T18:21:38.302328Z","shell.execute_reply":"2023-11-05T18:21:38.301306Z","shell.execute_reply.started":"2023-11-05T18:21:38.259997Z"},"trusted":true},"outputs":[],"source":["with open(VOCAB_DIRNAME + '/token_toxicities.txt', 'w') as f:\n","    for t in token_toxicities:\n","        f.write(str(t))\n","        f.write('\\n')"]},{"cell_type":"markdown","metadata":{},"source":["#### 3. Setting up the model"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:21:42.850896Z","iopub.status.busy":"2023-11-05T18:21:42.849867Z","iopub.status.idle":"2023-11-05T18:21:42.861872Z","shell.execute_reply":"2023-11-05T18:21:42.860888Z","shell.execute_reply.started":"2023-11-05T18:21:42.850856Z"},"trusted":true},"outputs":[],"source":["with open(VOCAB_DIRNAME + \"/negative-words.txt\", \"r\") as f:\n","    s = f.readlines()\n","negative_words = list(map(lambda x: x[:-1], s))\n","\n","with open(VOCAB_DIRNAME + \"/positive-words.txt\", \"r\") as f:\n","    s = f.readlines()\n","positive_words = list(map(lambda x: x[:-1], s))"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:21:45.645339Z","iopub.status.busy":"2023-11-05T18:21:45.644506Z","iopub.status.idle":"2023-11-05T18:21:45.730920Z","shell.execute_reply":"2023-11-05T18:21:45.730090Z","shell.execute_reply.started":"2023-11-05T18:21:45.645304Z"},"trusted":true},"outputs":[],"source":["import pickle\n","with open(VOCAB_DIRNAME + '/word2coef.pkl', 'rb') as f:\n","    word2coef = pickle.load(f)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:21:57.757431Z","iopub.status.busy":"2023-11-05T18:21:57.757072Z","iopub.status.idle":"2023-11-05T18:21:57.788413Z","shell.execute_reply":"2023-11-05T18:21:57.787515Z","shell.execute_reply.started":"2023-11-05T18:21:57.757400Z"},"trusted":true},"outputs":[],"source":["token_toxicities = []\n","with open(VOCAB_DIRNAME + '/token_toxicities.txt', 'r') as f:\n","    for line in f.readlines():\n","        token_toxicities.append(float(line))\n","token_toxicities = np.array(token_toxicities)\n","token_toxicities = np.maximum(0, np.log(1/(1/token_toxicities-1)))   # log odds ratio\n","\n","# discourage meaningless tokens\n","for tok in ['.', ',', '-']:\n","    token_toxicities[tokenizer.encode(tok)][1] = 3\n","\n","for tok in ['you']:\n","    token_toxicities[tokenizer.encode(tok)][1] = 0"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:22:04.017525Z","iopub.status.busy":"2023-11-05T18:22:04.016673Z","iopub.status.idle":"2023-11-05T18:22:05.755186Z","shell.execute_reply":"2023-11-05T18:22:05.754092Z","shell.execute_reply.started":"2023-11-05T18:22:04.017489Z"},"trusted":true},"outputs":[],"source":["def adjust_logits(logits, label=0):\n","    return logits - token_toxicities * 100 * (1 - 2 * label)\n","\n","predictor = MaskedTokenPredictorBert(model, tokenizer, max_len=250, device=device, label=0, contrast_penalty=0.0, logits_postprocessor=adjust_logits)\n","\n","editor = CondBertRewriter(\n","    model=model,\n","    tokenizer=tokenizer,\n","    device=device,\n","    neg_words=negative_words,\n","    pos_words=positive_words,\n","    word2coef=word2coef,\n","    token_toxicities=token_toxicities,\n","    predictor=predictor,\n",")"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:22:08.740165Z","iopub.status.busy":"2023-11-05T18:22:08.739456Z","iopub.status.idle":"2023-11-05T18:22:27.684452Z","shell.execute_reply":"2023-11-05T18:22:27.683602Z","shell.execute_reply.started":"2023-11-05T18:22:08.740128Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-11-05 18:22:09,282 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpnn88uuz2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 160000128/160000128 [00:08<00:00, 19695200.97B/s]"]},{"name":"stdout","output_type":"stream","text":["2023-11-05 18:22:18,080 copying /tmp/tmpnn88uuz2 to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n","2023-11-05 18:22:18,201 removing temp file /tmp/tmpnn88uuz2\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["2023-11-05 18:22:18,638 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmp8j940ugj\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 21494764/21494764 [00:01<00:00, 12222875.42B/s]"]},{"name":"stdout","output_type":"stream","text":["2023-11-05 18:22:20,813 copying /tmp/tmp8j940ugj to cache at /root/.flair/embeddings/glove.gensim\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["2023-11-05 18:22:20,832 removing temp file /tmp/tmp8j940ugj\n"]}],"source":["chooser = EmbeddingSimilarityChooser(sim_coef=10, tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4. Finally, the inference"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:22:49.423573Z","iopub.status.busy":"2023-11-05T18:22:49.423219Z","iopub.status.idle":"2023-11-05T18:22:52.061131Z","shell.execute_reply":"2023-11-05T18:22:52.060081Z","shell.execute_reply.started":"2023-11-05T18:22:49.423545Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["you are mistake !\n"]}],"source":["print(editor.translate('You are idiot!', prnt=False))"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:22:57.376487Z","iopub.status.busy":"2023-11-05T18:22:57.376073Z","iopub.status.idle":"2023-11-05T18:22:57.959402Z","shell.execute_reply":"2023-11-05T18:22:57.958444Z","shell.execute_reply.started":"2023-11-05T18:22:57.376454Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["you are so beautiful !\n"]}],"source":["print(editor.replacement_loop('You are stupid!', verbose=False, chooser=chooser, n_tokens=(1, 2, 3), n_top=10))"]},{"cell_type":"markdown","metadata":{},"source":["Parameters that could be tuned:\n","\n","* The coeffincient in adjust_logits - the larger it is, the more the model avoids toxic words\n","* The coefficient in EmbeddingSimilarityChooser - the larger it is, the more the model tries to preserve content\n","* n_tokens - how many words can be generated from one\n","* n_top - how many BERT hypotheses are reranked"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:29:20.444426Z","iopub.status.busy":"2023-11-05T18:29:20.444074Z","iopub.status.idle":"2023-11-05T18:29:20.581489Z","shell.execute_reply":"2023-11-05T18:29:20.580491Z","shell.execute_reply.started":"2023-11-05T18:29:20.444399Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data into training and test sets\n","train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-11-05T18:29:22.660262Z","iopub.status.busy":"2023-11-05T18:29:22.659863Z","iopub.status.idle":"2023-11-05T19:05:51.187383Z","shell.execute_reply":"2023-11-05T19:05:51.186434Z","shell.execute_reply.started":"2023-11-05T18:29:22.660226Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 115556/115556 [36:28<00:00, 52.80it/s]\n"]}],"source":["test_output = []\n","\n","lines = list(test_data['toxic'])\n","for i, tx in enumerate(tqdm(lines)):\n","    inp = tx.strip()\n","    out = editor.translate(inp, prnt=False)\n","    test_output += out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open('results.txt', 'w') as file:\n","    for item in test_output:\n","        file.write(\"%s\\n\" % item)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":4}
